{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n"
     ]
    }
   ],
   "source": [
    "from utils import load_cifar10_data\n",
    "x_train, y_train, x_validate, y_validate, x_test, y_test = load_cifar10_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network, train, utils\n",
    "from layers import ReluLayer, TernaryFullyConnectedLayer, \\\n",
    "    TernaryConvolutionLayer, BatchNormLayer, MaxPoolingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = network.NeuralNetwork(in_size=[None, 32, 32, 3], n_out_classes=10, \n",
    "                           loss_func=utils.smooth_hinge_loss)\n",
    "nn.reset_graph()\n",
    "\n",
    "# Hidden Conv-1\n",
    "nn.add_layer(TernaryConvolutionLayer(\n",
    "    out_dim=128, filter_size=3))\n",
    "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Conv-2\n",
    "nn.add_layer(TernaryConvolutionLayer(\n",
    "    out_dim=128, filter_size=3))\n",
    "nn.add_layer(MaxPoolingLayer())\n",
    "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Conv-3\n",
    "nn.add_layer(TernaryConvolutionLayer(\n",
    "    out_dim=256, filter_size=3))\n",
    "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Conv-4\n",
    "nn.add_layer(TernaryConvolutionLayer(\n",
    "    out_dim=256, filter_size=3))\n",
    "nn.add_layer(MaxPoolingLayer())\n",
    "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Conv-5\n",
    "nn.add_layer(TernaryConvolutionLayer(\n",
    "    out_dim=512, filter_size=3))\n",
    "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Conv-6\n",
    "nn.add_layer(TernaryConvolutionLayer(\n",
    "    out_dim=512, filter_size=3))\n",
    "nn.add_layer(MaxPoolingLayer())\n",
    "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Fc-7\n",
    "nn.add_layer(TernaryFullyConnectedLayer(\n",
    "    out_dim=1024))\n",
    "nn.add_layer(BatchNormLayer(axes=[0]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Fc-8\n",
    "nn.add_layer(TernaryFullyConnectedLayer(\n",
    "    out_dim=1024))\n",
    "nn.add_layer(BatchNormLayer(axes=[0]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Fc-9\n",
    "nn.add_layer(TernaryFullyConnectedLayer(out_dim=10))\n",
    "nn.add_layer(BatchNormLayer(axes=[0]))\n",
    "\n",
    "nn.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = (x_train, y_train)\n",
    "opt = train.Trainer(nn, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "opt.set_rho(0.25)\n",
    "opt.set_ema_rates(0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99900001, 0.99900001, 0.99900001, 0.99900001, 0.99900001, 0.99900001, 0.99900001, 0.99900001, 0.99900001]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 2.185514, 0.08\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 1.763269, 0.17\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 1.530774, 0.29\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 1.489351, 0.32\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 1.476776, 0.22\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 1.407553, 0.25\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 1.330751, 0.33\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 1.280346, 0.23\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 1.164009, 0.33\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 1.026044, 0.35\n",
      "Sparsity fraction (ratio of non-zero weights):  0.86653985013\n",
      "Train loss/acc:  (1.0487055619557699, 0.25993333220481873) Test loss/acc:  (1.0540155649185181, 0.25819999933242799)\n",
      "Epoch:  1\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99902004, 0.99902004, 0.99902004, 0.99902004, 0.99902004, 0.99902004, 0.99902004, 0.99902004, 0.99902004]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.967408, 0.28\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.906393, 0.42\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.936602, 0.29\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.860589, 0.35\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.770980, 0.45\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.738042, 0.48\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.695828, 0.44\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.592091, 0.59\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.580725, 0.55\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.566467, 0.52\n",
      "Sparsity fraction (ratio of non-zero weights):  0.827716754922\n",
      "Train loss/acc:  (0.58014761606852217, 0.45266666597790189) Test loss/acc:  (0.5874196863174439, 0.44379999637603762)\n",
      "Epoch:  2\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99903965, 0.99903965, 0.99903965, 0.99903965, 0.99903965, 0.99903965, 0.99903965, 0.99903965, 0.99903965]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.515304, 0.61\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.473341, 0.59\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.507224, 0.53\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.478309, 0.53\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.499571, 0.50\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.388184, 0.68\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.365544, 0.67\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.400334, 0.57\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.311778, 0.75\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.308073, 0.73\n",
      "Sparsity fraction (ratio of non-zero weights):  0.739873850618\n",
      "Train loss/acc:  (0.35507574571503531, 0.60126666943232221) Test loss/acc:  (0.37484725236892702, 0.57250000238418575)\n",
      "Epoch:  3\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99905884, 0.99905884, 0.99905884, 0.99905884, 0.99905884, 0.99905884, 0.99905884, 0.99905884, 0.99905884]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.289535, 0.75\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.247720, 0.76\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.268410, 0.74\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.249627, 0.73\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.215415, 0.81\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.208241, 0.81\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.235259, 0.68\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.195551, 0.81\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.187072, 0.83\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.185592, 0.79\n",
      "Sparsity fraction (ratio of non-zero weights):  0.612536134017\n",
      "Train loss/acc:  (0.46236982451544867, 0.44399999976158144) Test loss/acc:  (0.47401668190956114, 0.43320000171661377)\n",
      "Epoch:  4\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99907768, 0.99907768, 0.99907768, 0.99907768, 0.99907768, 0.99907768, 0.99907768, 0.99907768, 0.99907768]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.206641, 0.73\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.231244, 0.66\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.131631, 0.84\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.163746, 0.80\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.160933, 0.80\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.179580, 0.75\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.155434, 0.79\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.143923, 0.82\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.130502, 0.87\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.137264, 0.79\n",
      "Sparsity fraction (ratio of non-zero weights):  0.440226088682\n",
      "Train loss/acc:  (0.19843997140725453, 0.68997777753406098) Test loss/acc:  (0.22065262556076048, 0.652000002861023)\n",
      "Epoch:  5\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.9990961, 0.9990961, 0.9990961, 0.9990961, 0.9990961, 0.9990961, 0.9990961, 0.9990961, 0.9990961]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.109934, 0.91\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.102749, 0.86\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.106908, 0.83\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.094586, 0.87\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.093589, 0.88\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.177633, 0.77\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.110606, 0.86\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.103178, 0.85\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.112876, 0.82\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.160923, 0.76\n",
      "Sparsity fraction (ratio of non-zero weights):  0.218821464383\n",
      "Train loss/acc:  (0.17328559610578748, 0.74531111187405052) Test loss/acc:  (0.19685726284980773, 0.69779999971389772)\n",
      "Epoch:  6\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99911416, 0.99911416, 0.99911416, 0.99911416, 0.99911416, 0.99911416, 0.99911416, 0.99911416, 0.99911416]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.101360, 0.90\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.084349, 0.90\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.068800, 0.94\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.102798, 0.84\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.102011, 0.83\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.085982, 0.90\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.128998, 0.78\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.098152, 0.89\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.090720, 0.86\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.146896, 0.81\n",
      "Sparsity fraction (ratio of non-zero weights):  0.0933636860504\n",
      "Train loss/acc:  (0.12102858854664697, 0.80757778008778891) Test loss/acc:  (0.15044909119606018, 0.75099999904632564)\n",
      "Epoch:  7\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99913186, 0.99913186, 0.99913186, 0.99913186, 0.99913186, 0.99913186, 0.99913186, 0.99913186, 0.99913186]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.065634, 0.92\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.101577, 0.84\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.122443, 0.79\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.115263, 0.81\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.074701, 0.92\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.070499, 0.92\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.086770, 0.85\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.085493, 0.88\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.097044, 0.84\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.063649, 0.92\n",
      "Sparsity fraction (ratio of non-zero weights):  0.0811218131572\n",
      "Train loss/acc:  (0.1194810085164176, 0.80855555322435169) Test loss/acc:  (0.154729163646698, 0.74250000000000005)\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.9991492, 0.9991492, 0.9991492, 0.9991492, 0.9991492, 0.9991492, 0.9991492, 0.9991492, 0.9991492]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.095779, 0.88\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.095003, 0.84\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.061731, 0.92\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.073667, 0.89\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.056342, 0.92\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.054214, 0.95\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.061203, 0.94\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.081476, 0.88\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.082829, 0.88\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.092711, 0.88\n",
      "Sparsity fraction (ratio of non-zero weights):  0.0818494116346\n",
      "Train loss/acc:  (0.18607755740483603, 0.67142222245534267) Test loss/acc:  (0.21060839414596558, 0.63039999961853033)\n",
      "Epoch:  9\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99916619, 0.99916619, 0.99916619, 0.99916619, 0.99916619, 0.99916619, 0.99916619, 0.99916619, 0.99916619]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.066577, 0.92\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.052836, 0.94\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.074580, 0.87\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.052261, 0.95\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.053722, 0.95\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.099388, 0.86\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.059048, 0.91\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.075478, 0.87\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.046599, 0.94\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.076002, 0.91\n",
      "Sparsity fraction (ratio of non-zero weights):  0.083486080545\n",
      "Train loss/acc:  (0.10278730634186004, 0.83935555537541706) Test loss/acc:  (0.14268705725669861, 0.75929999589920039)\n",
      "Epoch:  10\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99918288, 0.99918288, 0.99918288, 0.99918288, 0.99918288, 0.99918288, 0.99918288, 0.99918288, 0.99918288]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.049968, 0.94\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.035814, 0.99\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.038387, 0.95\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.046409, 0.97\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.084662, 0.83\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.071542, 0.92\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.075846, 0.88\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.058592, 0.91\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.059784, 0.93\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.063819, 0.93\n",
      "Sparsity fraction (ratio of non-zero weights):  0.090750090736\n",
      "Train loss/acc:  (0.094511882695886826, 0.84646666765213008) Test loss/acc:  (0.13580133348703385, 0.77040000200271608)\n",
      "Epoch:  11\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99919921, 0.99919921, 0.99919921, 0.99919921, 0.99919921, 0.99919921, 0.99919921, 0.99919921, 0.99919921]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.060470, 0.91\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.048644, 0.93\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.034680, 0.98\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.050249, 0.93\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.057199, 0.91\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.050369, 0.92\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.058407, 0.93\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.060462, 0.91\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.045616, 0.94\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.054353, 0.91\n",
      "Sparsity fraction (ratio of non-zero weights):  0.0869239263289\n",
      "Train loss/acc:  (0.11670085940096113, 0.80842222240236072) Test loss/acc:  (0.16136432588100433, 0.7222000050544739)\n",
      "Epoch:  12\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99921525, 0.99921525, 0.99921525, 0.99921525, 0.99921525, 0.99921525, 0.99921525, 0.99921525, 0.99921525]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.030832, 0.96\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.038863, 0.97\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.042952, 0.96\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.046024, 0.95\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.046879, 0.93\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.052192, 0.95\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.048352, 0.92\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.044167, 0.94\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.040733, 0.95\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.054593, 0.93\n",
      "Sparsity fraction (ratio of non-zero weights):  0.0896351002401\n",
      "Train loss/acc:  (0.071861210895909203, 0.89179999510447183) Test loss/acc:  (0.13026966840028764, 0.79110000371932987)\n",
      "Epoch:  13\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99923092, 0.99923092, 0.99923092, 0.99923092, 0.99923092, 0.99923092, 0.99923092, 0.99923092, 0.99923092]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.040427, 0.97\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.027299, 0.98\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.030771, 0.97\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.023520, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.019947, 0.99\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.022150, 0.98\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.031836, 0.95\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.038967, 0.96\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.043371, 0.94\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.035085, 0.97\n",
      "Sparsity fraction (ratio of non-zero weights):  0.0916308640888\n",
      "Train loss/acc:  (0.08927049683199989, 0.85737777921888558) Test loss/acc:  (0.14526309639215471, 0.75970000267028803)\n",
      "Epoch:  14\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.9992463, 0.9992463, 0.9992463, 0.9992463, 0.9992463, 0.9992463, 0.9992463, 0.9992463, 0.9992463]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.025454, 0.96\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.030584, 0.97\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.017770, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.016685, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.028353, 0.97\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.019338, 0.99\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.031441, 0.98\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.045829, 0.94\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.060829, 0.91\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.034289, 0.98\n",
      "Sparsity fraction (ratio of non-zero weights):  0.0953072036526\n",
      "Train loss/acc:  (0.064936728411250644, 0.90197777642144095) Test loss/acc:  (0.12513008445501328, 0.78850000143051147)\n",
      "Epoch:  15\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99926138, 0.99926138, 0.99926138, 0.99926138, 0.99926138, 0.99926138, 0.99926138, 0.99926138, 0.99926138]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.016421, 0.99\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.040842, 0.94\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.017383, 0.99\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.020423, 0.98\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.022796, 0.99\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.015465, 0.99\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.041705, 0.94\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.042743, 0.95\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.026969, 0.98\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.035806, 0.97\n",
      "Sparsity fraction (ratio of non-zero weights):  0.0984441880363\n",
      "Train loss/acc:  (0.13752985854943595, 0.76388888676961264) Test loss/acc:  (0.19133038341999054, 0.6849999976158142)\n",
      "Epoch:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99927616, 0.99927616, 0.99927616, 0.99927616, 0.99927616, 0.99927616, 0.99927616, 0.99927616, 0.99927616]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.023897, 0.98\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.030180, 0.97\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.021685, 0.98\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.023616, 0.97\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.013639, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.023015, 0.98\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.037142, 0.95\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.015709, 0.99\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.024261, 0.98\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.054330, 0.93\n",
      "Sparsity fraction (ratio of non-zero weights):  0.100533253845\n",
      "Train loss/acc:  (0.05806496096981896, 0.91502222749922013) Test loss/acc:  (0.12391566842794419, 0.79860000133514408)\n",
      "Epoch:  17\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99929065, 0.99929065, 0.99929065, 0.99929065, 0.99929065, 0.99929065, 0.99929065, 0.99929065, 0.99929065]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.021013, 0.99\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.026408, 0.99\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.023411, 0.98\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.020901, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.029412, 0.95\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.009328, 0.99\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.037703, 0.94\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.023248, 0.97\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.047598, 0.93\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.033684, 0.97\n",
      "Sparsity fraction (ratio of non-zero weights):  0.103027673547\n",
      "Train loss/acc:  (0.095571795072820445, 0.84704444752799135) Test loss/acc:  (0.16066755294799806, 0.74120000600814817)\n",
      "Epoch:  18\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99930483, 0.99930483, 0.99930483, 0.99930483, 0.99930483, 0.99930483, 0.99930483, 0.99930483, 0.99930483]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.036971, 0.95\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.011908, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.012384, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.022633, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.039557, 0.97\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.024833, 0.98\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.011605, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.033118, 0.95\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.026590, 0.99\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.029471, 0.95\n",
      "Sparsity fraction (ratio of non-zero weights):  0.105919321532\n",
      "Train loss/acc:  (0.066891292035579683, 0.89464444054497616) Test loss/acc:  (0.14131354242563249, 0.77559999227523801)\n",
      "Epoch:  19\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99931872, 0.99931872, 0.99931872, 0.99931872, 0.99931872, 0.99931872, 0.99931872, 0.99931872, 0.99931872]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.008430, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.006895, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.011350, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.023412, 0.98\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.008766, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.013450, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.013758, 0.98\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.025475, 0.98\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.039453, 0.96\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.031930, 0.99\n",
      "Sparsity fraction (ratio of non-zero weights):  0.107221272334\n",
      "Train loss/acc:  (0.058115599205096562, 0.91797778367996219) Test loss/acc:  (0.12967491388320923, 0.79520000219345088)\n",
      "Epoch:  20\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99933237, 0.99933237, 0.99933237, 0.99933237, 0.99933237, 0.99933237, 0.99933237, 0.99933237, 0.99933237]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.021838, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.009410, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.010689, 0.99\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.010498, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.014647, 0.99\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.016043, 0.99\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.029223, 0.98\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.022477, 0.99\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.020409, 0.99\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.043926, 0.90\n",
      "Sparsity fraction (ratio of non-zero weights):  0.110940235711\n",
      "Train loss/acc:  (0.046715446578131779, 0.9360444458325704) Test loss/acc:  (0.12190005660057068, 0.80650000095367436)\n",
      "Epoch:  21\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99934572, 0.99934572, 0.99934572, 0.99934572, 0.99934572, 0.99934572, 0.99934572, 0.99934572, 0.99934572]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.019848, 0.96\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.024799, 0.97\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.010388, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.022085, 0.98\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.014368, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.016434, 0.99\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.008274, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.015573, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.035586, 0.96\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.024479, 0.97\n",
      "Sparsity fraction (ratio of non-zero weights):  0.111151430293\n",
      "Train loss/acc:  (0.063021993901994491, 0.90046666357252336) Test loss/acc:  (0.13943614721298217, 0.77550000190734858)\n",
      "Epoch:  22\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99935883, 0.99935883, 0.99935883, 0.99935883, 0.99935883, 0.99935883, 0.99935883, 0.99935883, 0.99935883]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.007786, 0.99\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.021090, 0.99\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.004753, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.019411, 0.97\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.020259, 0.98\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.027930, 0.96\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.013085, 0.99\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.029334, 0.96\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.022925, 0.99\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.026855, 0.97\n",
      "Sparsity fraction (ratio of non-zero weights):  0.113786979009\n",
      "Train loss/acc:  (0.076009956863191397, 0.87686666541629366) Test loss/acc:  (0.15367767095565796, 0.75659999847412107)\n",
      "Epoch:  23\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99937165, 0.99937165, 0.99937165, 0.99937165, 0.99937165, 0.99937165, 0.99937165, 0.99937165, 0.99937165]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.013853, 0.99\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.017303, 0.99\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.011742, 0.99\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.006502, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.013048, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.007945, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.009736, 0.99\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.005940, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.036502, 0.96\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.041746, 0.95\n",
      "Sparsity fraction (ratio of non-zero weights):  0.114559339619\n",
      "Train loss/acc:  (0.075215107666121592, 0.87786666499243837) Test loss/acc:  (0.15194104492664337, 0.75399999856948852)\n",
      "Epoch:  24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99938422, 0.99938422, 0.99938422, 0.99938422, 0.99938422, 0.99938422, 0.99938422, 0.99938422, 0.99938422]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.013520, 0.99\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.018849, 0.98\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.005033, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.010374, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.002493, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.022601, 0.97\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.012406, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.028634, 0.97\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.031286, 0.97\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.029178, 0.96\n",
      "Sparsity fraction (ratio of non-zero weights):  0.114427120264\n",
      "Train loss/acc:  (0.066314009063773688, 0.89493332703908279) Test loss/acc:  (0.14940252006053925, 0.75999999523162842)\n",
      "Epoch:  25\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99939656, 0.99939656, 0.99939656, 0.99939656, 0.99939656, 0.99939656, 0.99939656, 0.99939656, 0.99939656]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.015652, 0.99\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.013771, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.027791, 0.98\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.015511, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.006966, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.010111, 0.99\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.010769, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.011054, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.006291, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.020889, 0.98\n",
      "Sparsity fraction (ratio of non-zero weights):  0.114537172385\n",
      "Train loss/acc:  (0.040037007745769286, 0.94399999433093595) Test loss/acc:  (0.12143065303564071, 0.80759999752044676)\n",
      "Epoch:  26\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.9994086, 0.9994086, 0.9994086, 0.9994086, 0.9994086, 0.9994086, 0.9994086, 0.9994086, 0.9994086]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.005913, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.010184, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.017917, 0.98\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.009368, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.017866, 0.98\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.016456, 0.98\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.008352, 0.99\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.022568, 0.98\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.018385, 0.99\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.011863, 0.99\n",
      "Sparsity fraction (ratio of non-zero weights):  0.114796265299\n",
      "Train loss/acc:  (0.038229279369115826, 0.94417777352862886) Test loss/acc:  (0.12945931673049926, 0.80299999952316281)\n",
      "Epoch:  27\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.9994204, 0.9994204, 0.9994204, 0.9994204, 0.9994204, 0.9994204, 0.9994204, 0.9994204, 0.9994204]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.010001, 0.99\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.015610, 0.98\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.004428, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.019428, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.010004, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.017426, 0.97\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.005623, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.024758, 0.98\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.024423, 0.98\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.004173, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.115459642946\n",
      "Train loss/acc:  (0.037969853166076872, 0.94437777174843685) Test loss/acc:  (0.12600858181715011, 0.80680000066757207)\n",
      "Epoch:  28\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99943197, 0.99943197, 0.99943197, 0.99943197, 0.99943197, 0.99943197, 0.99943197, 0.99943197, 0.99943197]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.013312, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.013553, 0.99\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.012240, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.004742, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.008367, 0.99\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.020197, 0.98\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.008985, 0.99\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.018462, 0.98\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.034642, 0.98\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.018960, 0.99\n",
      "Sparsity fraction (ratio of non-zero weights):  0.117034229346\n",
      "Train loss/acc:  (0.052815960446993507, 0.91948889732360839) Test loss/acc:  (0.14294432938098908, 0.78230000019073487)\n",
      "Epoch:  29\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99944335, 0.99944335, 0.99944335, 0.99944335, 0.99944335, 0.99944335, 0.99944335, 0.99944335, 0.99944335]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.010030, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.012642, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.006498, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.011820, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.001727, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.007162, 0.99\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.009136, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.014964, 0.98\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.013932, 0.99\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.008831, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.117375219855\n",
      "Train loss/acc:  (0.079054866797394222, 0.87393333382076688) Test loss/acc:  (0.17689713895320891, 0.74620000362396244)\n",
      "Epoch:  30\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.9994545, 0.9994545, 0.9994545, 0.9994545, 0.9994545, 0.9994545, 0.9994545, 0.9994545, 0.9994545]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.008561, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.015394, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.014797, 0.98\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.010083, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.004148, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.007623, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.009105, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.007768, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.015707, 0.99\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.018060, 0.98\n",
      "Sparsity fraction (ratio of non-zero weights):  0.117452341872\n",
      "Train loss/acc:  (0.037906896273295083, 0.94635554764005869) Test loss/acc:  (0.13406919717788696, 0.80160000801086428)\n",
      "Epoch:  31\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99946541, 0.99946541, 0.99946541, 0.99946541, 0.99946541, 0.99946541, 0.99946541, 0.99946541, 0.99946541]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.010046, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.007093, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.011657, 0.99\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.005884, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.012350, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.010874, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.017954, 0.99\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.007996, 0.99\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.017099, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.015273, 0.99\n",
      "Sparsity fraction (ratio of non-zero weights):  0.11739446472\n",
      "Train loss/acc:  (0.033097471098105111, 0.95386665423711137) Test loss/acc:  (0.12743649125099182, 0.80660000085830685)\n",
      "Epoch:  32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99947608, 0.99947608, 0.99947608, 0.99947608, 0.99947608, 0.99947608, 0.99947608, 0.99947608, 0.99947608]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.003824, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.009753, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.003500, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.017965, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.005685, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.016483, 0.98\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.007077, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.010490, 0.99\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.011679, 0.99\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.017341, 0.98\n",
      "Sparsity fraction (ratio of non-zero weights):  0.118082789416\n",
      "Train loss/acc:  (0.057745295729902058, 0.90917778068118626) Test loss/acc:  (0.15464711844921111, 0.76380000114440916)\n",
      "Epoch:  33\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99948657, 0.99948657, 0.99948657, 0.99948657, 0.99948657, 0.99948657, 0.99948657, 0.99948657, 0.99948657]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.005196, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.009285, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.009059, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.021096, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.017593, 0.98\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.003177, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.005832, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.005599, 0.99\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.023240, 0.98\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.010522, 0.99\n",
      "Sparsity fraction (ratio of non-zero weights):  0.118350079218\n",
      "Train loss/acc:  (0.045273651861482196, 0.93206667078865901) Test loss/acc:  (0.14895420104265214, 0.78779999732971195)\n",
      "Epoch:  34\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99949682, 0.99949682, 0.99949682, 0.99949682, 0.99949682, 0.99949682, 0.99949682, 0.99949682, 0.99949682]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.011180, 0.98\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.007415, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.007598, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.005955, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.012457, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.002525, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.009158, 0.99\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.007512, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.018723, 0.97\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.036456, 0.94\n",
      "Sparsity fraction (ratio of non-zero weights):  0.118156846511\n",
      "Train loss/acc:  (0.078054593867725799, 0.87917777379353845) Test loss/acc:  (0.17924500524997711, 0.74940000057220457)\n",
      "Epoch:  35\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99950689, 0.99950689, 0.99950689, 0.99950689, 0.99950689, 0.99950689, 0.99950689, 0.99950689, 0.99950689]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.004190, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.007758, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.009023, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.007793, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.005211, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.013632, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.004077, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.016355, 0.99\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.010427, 0.99\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.007452, 0.99\n",
      "Sparsity fraction (ratio of non-zero weights):  0.117713287995\n",
      "Train loss/acc:  (0.031231415818134944, 0.95491109768549598) Test loss/acc:  (0.13168337643146516, 0.80840000152587888)\n",
      "Epoch:  36\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99951673, 0.99951673, 0.99951673, 0.99951673, 0.99951673, 0.99951673, 0.99951673, 0.99951673, 0.99951673]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.006040, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.005371, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.010772, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.007073, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.018872, 0.99\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.012424, 0.98\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.011526, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.004923, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.003972, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.007091, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.116743845706\n",
      "Train loss/acc:  (0.024779367074370385, 0.96584444178475271) Test loss/acc:  (0.12512974113225936, 0.81639999628067017)\n",
      "Epoch:  37\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99952638, 0.99952638, 0.99952638, 0.99952638, 0.99952638, 0.99952638, 0.99952638, 0.99952638, 0.99952638]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.005532, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.013749, 0.98\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.012596, 0.99\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.004712, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.002748, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.003701, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.010981, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.008840, 0.99\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.031879, 0.96\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.002719, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.116607135882\n",
      "Train loss/acc:  (0.027216567049423854, 0.96171110285653005) Test loss/acc:  (0.12864022880792617, 0.8173000001907349)\n",
      "Epoch:  38\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99953586, 0.99953586, 0.99953586, 0.99953586, 0.99953586, 0.99953586, 0.99953586, 0.99953586, 0.99953586]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.007149, 0.99\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.002056, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.009146, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.020592, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.004540, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.010937, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.012617, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.004998, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.011387, 0.99\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.006585, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.116637357449\n",
      "Train loss/acc:  (0.044877232909202577, 0.93157778236601085) Test loss/acc:  (0.15004192203283309, 0.78980000495910641)\n",
      "Epoch:  39\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99954516, 0.99954516, 0.99954516, 0.99954516, 0.99954516, 0.99954516, 0.99954516, 0.99954516, 0.99954516]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.002628, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.010443, 0.99\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.004470, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.005978, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.005409, 0.99\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.003050, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.001914, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.010071, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.013299, 0.99\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.018411, 0.97\n",
      "Sparsity fraction (ratio of non-zero weights):  0.117414493636\n",
      "Train loss/acc:  (0.031716946793927087, 0.9546888788541158) Test loss/acc:  (0.1352580890059471, 0.80790000200271606)\n",
      "Epoch:  40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99955428, 0.99955428, 0.99955428, 0.99955428, 0.99955428, 0.99955428, 0.99955428, 0.99955428, 0.99955428]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.002984, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.001142, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.005725, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.007303, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.009546, 0.99\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.001470, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.005081, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.001859, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.007992, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.003973, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.115592076133\n",
      "Train loss/acc:  (0.03024781902631124, 0.9565555432107713) Test loss/acc:  (0.13848123371601104, 0.80430000066757201)\n",
      "Epoch:  41\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99956322, 0.99956322, 0.99956322, 0.99956322, 0.99956322, 0.99956322, 0.99956322, 0.99956322, 0.99956322]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.003917, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.006330, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.001315, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.004871, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.002847, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.008875, 0.99\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.003930, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.012727, 0.99\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.009543, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.008644, 0.99\n",
      "Sparsity fraction (ratio of non-zero weights):  0.113779138509\n",
      "Train loss/acc:  (0.02550029728975561, 0.96399999221165977) Test loss/acc:  (0.13115064084529876, 0.81120000600814823)\n",
      "Epoch:  42\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99957198, 0.99957198, 0.99957198, 0.99957198, 0.99957198, 0.99957198, 0.99957198, 0.99957198, 0.99957198]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.002882, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.008818, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.005335, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.009298, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.007014, 0.99\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.005612, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.009838, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.018534, 0.99\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.000793, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.003598, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.114496758167\n",
      "Train loss/acc:  (0.040187640024556055, 0.93904444350136651) Test loss/acc:  (0.15413572877645493, 0.78879999637603759)\n",
      "Epoch:  43\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99958056, 0.99958056, 0.99958056, 0.99958056, 0.99958056, 0.99958056, 0.99958056, 0.99958056, 0.99958056]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.010402, 0.99\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.014481, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.013752, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.005100, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.010432, 0.98\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.004090, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.006105, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.010533, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.008280, 0.99\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.006676, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.116226729037\n",
      "Train loss/acc:  (0.024625843225253952, 0.96453332980473838) Test loss/acc:  (0.13247329384088516, 0.81069999217987065)\n",
      "Epoch:  44\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99958897, 0.99958897, 0.99958897, 0.99958897, 0.99958897, 0.99958897, 0.99958897, 0.99958897, 0.99958897]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.001102, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.006995, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.009441, 0.99\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.010430, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.006009, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.002803, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.010496, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.007281, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.005328, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.005389, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.113733734881\n",
      "Train loss/acc:  (0.026276687127020623, 0.96262221336364751) Test loss/acc:  (0.13063063889741897, 0.81619999408721922)\n",
      "Epoch:  45\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99959719, 0.99959719, 0.99959719, 0.99959719, 0.99959719, 0.99959719, 0.99959719, 0.99959719, 0.99959719]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.002562, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.010207, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.004310, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.001906, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.005658, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.012540, 0.98\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.016955, 0.98\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.008939, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.002055, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.001481, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.112902356683\n",
      "Train loss/acc:  (0.022553784565793142, 0.96822222153345738) Test loss/acc:  (0.13501640796661377, 0.8204000067710876)\n",
      "Epoch:  46\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99960524, 0.99960524, 0.99960524, 0.99960524, 0.99960524, 0.99960524, 0.99960524, 0.99960524, 0.99960524]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.002058, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.000559, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.004092, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.005673, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.009899, 0.99\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.000835, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.009456, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.004829, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.004067, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.007893, 0.99\n",
      "Sparsity fraction (ratio of non-zero weights):  0.114028038771\n",
      "Train loss/acc:  (0.025354375756449169, 0.96399999141693116) Test loss/acc:  (0.13939430058002472, 0.80850000143051148)\n",
      "Epoch:  47\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99961311, 0.99961311, 0.99961311, 0.99961311, 0.99961311, 0.99961311, 0.99961311, 0.99961311, 0.99961311]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.004802, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.014933, 0.98\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.004588, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.003380, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.011533, 0.99\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.006916, 0.99\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.007857, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.003207, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.001471, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.008416, 0.99\n",
      "Sparsity fraction (ratio of non-zero weights):  0.112875841535\n",
      "Train loss/acc:  (0.032110184050268595, 0.95286665466096665) Test loss/acc:  (0.14262020200490952, 0.80350000619888307)\n",
      "Epoch:  48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99962085, 0.99962085, 0.99962085, 0.99962085, 0.99962085, 0.99962085, 0.99962085, 0.99962085, 0.99962085]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.005369, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.007039, 0.99\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.006525, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.008316, 0.99\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.002607, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.006516, 0.99\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.002723, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.011644, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.015487, 0.99\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.003241, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.110037722788\n",
      "Train loss/acc:  (0.020162395321660573, 0.97284445338779024) Test loss/acc:  (0.13102664947509765, 0.81710000038146968)\n",
      "Epoch:  49\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99962842, 0.99962842, 0.99962842, 0.99962842, 0.99962842, 0.99962842, 0.99962842, 0.99962842, 0.99962842]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.004582, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.001526, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.005897, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.003680, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.001780, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.007498, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.015652, 0.98\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.001372, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.008921, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.010593, 0.99\n",
      "Sparsity fraction (ratio of non-zero weights):  0.110072149714\n",
      "Train loss/acc:  (0.033395277924007837, 0.951133324570126) Test loss/acc:  (0.15138222068548202, 0.79839999675750728)\n",
      "Epoch:  50\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99963588, 0.99963588, 0.99963588, 0.99963588, 0.99963588, 0.99963588, 0.99963588, 0.99963588, 0.99963588]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.011084, 0.99\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.026459, 0.97\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.012857, 0.98\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.010046, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.005074, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.020928, 0.99\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.006329, 0.99\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.003754, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.002124, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.005935, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.113785767296\n",
      "Train loss/acc:  (0.022734188305007088, 0.96857777727974781) Test loss/acc:  (0.12766704767942427, 0.81659999608993528)\n",
      "Epoch:  51\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99964315, 0.99964315, 0.99964315, 0.99964315, 0.99964315, 0.99964315, 0.99964315, 0.99964315, 0.99964315]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.001169, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.009253, 0.99\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.005824, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.016883, 0.98\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.004031, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.014314, 0.99\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.021087, 0.99\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.013729, 0.99\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.001226, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.002846, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.11276878306\n",
      "Train loss/acc:  (0.015436492322219744, 0.98026668442620168) Test loss/acc:  (0.12465616554021836, 0.82639999389648433)\n",
      "Epoch:  52\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.9996503, 0.9996503, 0.9996503, 0.9996503, 0.9996503, 0.9996503, 0.9996503, 0.9996503, 0.9996503]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.003001, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.005632, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.003283, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.005659, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.005116, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.002593, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.005411, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.006685, 0.99\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.006359, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.001297, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.107576589576\n",
      "Train loss/acc:  (0.027957782364553874, 0.96037776814566722) Test loss/acc:  (0.16033146411180496, 0.80379999876022334)\n",
      "Epoch:  53\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99965727, 0.99965727, 0.99965727, 0.99965727, 0.99965727, 0.99965727, 0.99965727, 0.99965727, 0.99965727]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.002605, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.005081, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.001984, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.004364, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.005117, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.008808, 0.99\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.004110, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.010117, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.003851, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.005344, 0.99\n",
      "Sparsity fraction (ratio of non-zero weights):  0.105561509584\n",
      "Train loss/acc:  (0.031233311601810986, 0.95484443320168388) Test loss/acc:  (0.15684079945087434, 0.80140000820159907)\n",
      "Epoch:  54\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99966413, 0.99966413, 0.99966413, 0.99966413, 0.99966413, 0.99966413, 0.99966413, 0.99966413, 0.99966413]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.002262, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.007738, 1.00\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.011286, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.004071, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.005165, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.001589, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.008884, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.002687, 1.00\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.005754, 1.00\n",
      "Iter: 405 of 450 || Estimated train loss/acc: 0.007434, 1.00\n",
      "Sparsity fraction (ratio of non-zero weights):  0.108656155264\n",
      "Train loss/acc:  (0.021932223894529871, 0.96977778302298656) Test loss/acc:  (0.14192571371793747, 0.81829999208450321)\n",
      "Epoch:  55\n",
      "========= Begin epoch =========\n",
      "batch_size = 100\n",
      "EMA rates:\n",
      "[0.99967086, 0.99967086, 0.99967086, 0.99967086, 0.99967086, 0.99967086, 0.99967086, 0.99967086, 0.99967086]\n",
      "rho:\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "Iter: 0 of 450 || Estimated train loss/acc: 0.003148, 1.00\n",
      "Iter: 45 of 450 || Estimated train loss/acc: 0.004923, 0.99\n",
      "Iter: 90 of 450 || Estimated train loss/acc: 0.007490, 1.00\n",
      "Iter: 135 of 450 || Estimated train loss/acc: 0.003620, 1.00\n",
      "Iter: 180 of 450 || Estimated train loss/acc: 0.003014, 1.00\n",
      "Iter: 225 of 450 || Estimated train loss/acc: 0.005996, 1.00\n",
      "Iter: 270 of 450 || Estimated train loss/acc: 0.006648, 1.00\n",
      "Iter: 315 of 450 || Estimated train loss/acc: 0.010245, 0.98\n",
      "Iter: 360 of 450 || Estimated train loss/acc: 0.000932, 1.00\n"
     ]
    }
   ],
   "source": [
    "losses_and_accs_train = []\n",
    "losses_and_accs_valid = []\n",
    "losses_and_accs_test = []\n",
    "\n",
    "sparsity_fracs = []\n",
    "\n",
    "n_epochs = 500\n",
    "\n",
    "for t in range(n_epochs):    \n",
    "    print('Epoch: ', t)\n",
    "\n",
    "    opt.train_epoch(batch_size=100, ema_decay=0.98, n_output=10, verbose=True)\n",
    "\n",
    "\n",
    "    losses_and_accs_train.append(\n",
    "        opt.loss_and_accuracy((x_train, y_train), max_batch=400, inference=True))\n",
    "    losses_and_accs_test.append(\n",
    "        opt.loss_and_accuracy((x_test, y_test), max_batch=400, inference=True))\n",
    "    losses_and_accs_valid.append(\n",
    "        opt.loss_and_accuracy((x_validate, y_validate), max_batch=400, inference=True))\n",
    "    \n",
    "    sparsity_fracs.append(utils.get_sparsity_frac(nn, opt))\n",
    "    \n",
    "    print('Train loss/acc: ', losses_and_accs_train[-1],\n",
    "          'Test loss/acc: ', losses_and_accs_test[-1])\n",
    "    \n",
    "losses_and_accs_train = np.asarray(losses_and_accs_train)\n",
    "losses_and_accs_valid = np.asarray(losses_and_accs_valid)\n",
    "losses_and_accs_test = np.asarray(losses_and_accs_test)\n",
    "sparsity_fracs = np.asarray(sparsity_fracs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (1.487573415322648e-06, 1.0)\n",
      "Valid:  (0.24831801891326905, 0.86840000152587893)\n",
      "Test:  (0.26798239350318909, 0.8642000031471252)\n"
     ]
    }
   ],
   "source": [
    "print('Train: ', opt.loss_and_accuracy((x_train, y_train), inference=True,\n",
    "                                       max_batch=400))\n",
    "print('Valid: ', opt.loss_and_accuracy((x_validate, y_validate), inference=True,\n",
    "                                      max_batch=400))\n",
    "print('Test: ', opt.loss_and_accuracy((x_test, y_test), inference=True,\n",
    "                                     max_batch=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch:  141\n",
      "Train acc:  1.0\n",
      "Valid acc:  0.876199998856\n",
      "Test acc:  0.866100001335\n"
     ]
    }
   ],
   "source": [
    "best_epoch = np.argmax(losses_and_accs_valid[:,1]) + 1\n",
    "print('Best epoch: ', best_epoch)\n",
    "print('Train acc: ', losses_and_accs_train[best_epoch-1, 1])\n",
    "print('Valid acc: ', losses_and_accs_valid[best_epoch-1, 1])\n",
    "print('Test acc: ', losses_and_accs_test[best_epoch-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:  [  1.36232333e-06   1.00000000e+00   2.49434514e-01   8.67000003e-01\n",
      "   2.66178123e-01   8.64200006e-01]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbab02f48d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save data\n",
    "losses_and_accs = np.concatenate(\n",
    "    [np.asarray(losses_and_accs_train),\n",
    "     np.asarray(losses_and_accs_valid),\n",
    "     np.asarray(losses_and_accs_test)], axis=1)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\n",
    "ax1.semilogy(losses_and_accs[:,0], '-o', label='Train loss')\n",
    "ax1.semilogy(losses_and_accs[:,2], '-o', label='Valid loss')\n",
    "ax1.semilogy(losses_and_accs[:,4], '-o', label='Test loss')\n",
    "\n",
    "ax2.plot(losses_and_accs[:,1], '-o', label='Train acc')\n",
    "ax2.plot(losses_and_accs[:,3], '-o', label='Valid acc')\n",
    "ax2.plot(losses_and_accs[:,5], '-o', label='Test acc')\n",
    "\n",
    "for ax in [ax1,ax2]:\n",
    "    ax.legend()\n",
    "\n",
    "ax2.set_ylim(0.1,1)\n",
    "    \n",
    "print('Final results: ', losses_and_accs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sparsity fraction:  0.0129810895673\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbab0221438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sparsity fracs\n",
    "plt.plot(sparsity_fracs)\n",
    "print('Final sparsity fraction: ', sparsity_fracs[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
